# ==================== [1] TF é¢„åˆå§‹åŒ– ====================
try:
    from utils.mnist_loader import load_mnist_data
    print(">>> [System] é¢„åŠ è½½ MNIST æ•°æ®...")
    _ = load_mnist_data()
    _TF_PREINIT_SUCCESS = True
except:
    _TF_PREINIT_SUCCESS = False
# =========================================================

import jax
import jax.numpy as jnp
import numpy as np
import optax
import flax
from omegaconf import OmegaConf
from tqdm import tqdm
from functools import partial
import os
from typing import Any, Tuple, Dict

from networks import NETWORKS
from envs.mnist_env import MnistEnv
from brax.envs import wrappers
from brax.training.acme import running_statistics
from brax.training.acme import specs
from utils.functions import mean_weight_abs

# ==================== é…ç½®ç±» ====================
@flax.struct.dataclass
class ESConfig:
    network_cls: Any = None
    optim_cls:   Any = None
    env_cls:     Any = None
    pop_size:       int = 2048
    lr:           float = 0.1
    eps:          float = 1e-3
    weight_decay: float = 0.
    warmup_steps:   int = 0
    eval_size:      int = 128
    action_dtype: Any   = jnp.float32
    p_dtype:       Any  = jnp.float32
    network_dtype: Any  = jnp.float32
    clip_action:   bool = False
    normalize_obs: bool = False

@flax.struct.dataclass
class RunnerState:
    key: Any
    normalizer_state: running_statistics.RunningStatisticsState
    env_reset_pool: Any
    params:        Any
    fixed_weights: Any
    opt_state:     Any

@flax.struct.dataclass
class PopulationState:
    network_params: Any
    network_states: Any
    env_states:     Any
    fitness_totrew: jnp.ndarray
    fitness_sum:    jnp.ndarray
    fitness_n:      jnp.ndarray

# ==================== è¾…åŠ©å‡½æ•° ====================
def _centered_rank_transform(x):
    shape = x.shape
    x = x.ravel()
    x = jnp.argsort(jnp.argsort(x))
    x = x / (len(x) - 1) - .5
    return x.reshape(shape)

def _sample_bernoulli_parameter(key, params, dtype, batch_size=()):
    num_vars = len(jax.tree_util.tree_leaves(params))
    treedef = jax.tree_util.tree_structure(params)
    all_keys = jax.random.split(key, num=num_vars)
    return jax.tree_util.tree_map(
        lambda p, k: jax.random.uniform(k, (*batch_size, *p.shape), dtype) < p,
        params, jax.tree_util.tree_unflatten(treedef, all_keys))

def _deterministic_bernoulli_parameter(params, batch_size=()):
    return jax.tree_util.tree_map(lambda p: jnp.broadcast_to(p > 0.5, (*batch_size, *p.shape)), params)

# ==================== è¯„ä¼°æ­¥éª¤ ====================
def _evaluate_step(pop, runner, conf):
    vmapped_apply = jax.vmap(conf.network_cls.apply, ({"params": 0, "fixed_weights": None}, 0, 0))
    
    new_network_states, act = vmapped_apply(
        {"params": pop.network_params, "fixed_weights": runner.fixed_weights}, 
        pop.network_states, pop.env_states.obs
    )
    
    # ç¦ç”¨ Clip
    # act = jnp.clip(act, -1, 1) 
        
    new_env_states = conf.env_cls.step(pop.env_states, act)
    
    new_fitness_totrew = pop.fitness_totrew + new_env_states.reward
    new_fitness_sum    = jnp.where(new_env_states.done, pop.fitness_sum + new_fitness_totrew, pop.fitness_sum)
    new_fitness_n      = jnp.where(new_env_states.done, pop.fitness_n + 1, pop.fitness_n)
    new_fitness_totrew = jnp.where(new_env_states.done, 0, new_fitness_totrew)
    
    return pop.replace(
        network_states=new_network_states,
        env_states=new_env_states,
        fitness_totrew=new_fitness_totrew,
        fitness_sum=new_fitness_sum,
        fitness_n=new_fitness_n
    )

# ==================== [æ ¸å¿ƒä¿®æ”¹] è®­ç»ƒä¸€ä»£ (Run Generation) ====================
@partial(jax.jit, donate_argnums=(0,), static_argnums=(1, 2))
def _run_generation(runner, conf, eval_batch_indices):
    """
    eval_batch_indices: æœ¬ä»£è¦è¯„ä¼°çš„å›¾ç‰‡ç´¢å¼•åˆ—è¡¨ (Device Array)
    """
    new_key, run_key, carry_key = jax.random.split(runner.key, 3)
    runner = runner.replace(key=new_key)

    # 1. é‡‡æ ·ç§ç¾¤å‚æ•°
    train_params = _sample_bernoulli_parameter(run_key, runner.params, conf.network_dtype, (conf.pop_size - conf.eval_size, ))
    eval_params  = _deterministic_bernoulli_parameter(runner.params, (conf.eval_size, ))
    network_params = jax.tree_map(lambda t, e: jnp.concatenate([t, e], axis=0), train_params, eval_params)

    # åˆå§‹åŒ–ç´¯è®¡ Fitness
    total_fitness = jnp.zeros(conf.pop_size)
    total_eval_n = jnp.zeros(conf.pop_size, dtype=jnp.int32)

    # 2. [å¾ªç¯è¯„ä¼°] éå† eval_batch_indices ä¸­çš„æ¯ä¸€å¼ å›¾ç‰‡
    # scan çš„ carry æ˜¯ (total_fitness, total_eval_n)
    # scan çš„ x æ˜¯ image_index
    def _eval_one_image(carry, img_idx):
        fit_sum, fit_n = carry
        
        # æ„é€ æœ¬è½®çš„ç¯å¢ƒçŠ¶æ€ (å¼ºåˆ¶æ‰€æœ‰äººçœ‹åŒä¸€å¼ å›¾ img_idx)
        # æ³¨æ„: MnistEnv.reset å®é™…ä¸Šæ˜¯éšæœºå–å›¾ã€‚
        # æˆ‘ä»¬éœ€è¦ä¸€ç§æ–¹æ³•æŒ‡å®š indexã€‚
        # HACK: æˆ‘ä»¬ä¼ å…¥ä¸€ä¸ªç‰¹æ®Šçš„ seedï¼Œè®© Env å†…éƒ¨è§£æå‡º index?
        # ä¸ï¼Œæœ€å¥½çš„åŠæ³•æ˜¯ç›´æ¥è°ƒç”¨ env.reset ä½†ä¼ å…¥ç‰¹å®šçš„ keyï¼Œ
        # æˆ–è€…æˆ‘ä»¬ä¿®æ”¹ Env è®©å®ƒæ”¯æŒæŒ‡å®š Indexã€‚
        # ä¸ºäº†ä¸æ”¹ Envï¼Œæˆ‘ä»¬è¿™é‡Œç”¨ä¸€ç§ç®€åŒ–çš„å‡è®¾ï¼š
        # æˆ‘ä»¬å‡è®¾å¤–éƒ¨å·²ç»å‡†å¤‡å¥½äº† env_states (é€šè¿‡ hack reset)ï¼Œæˆ–è€…
        # æˆ‘ä»¬åˆ©ç”¨ Env çš„ reset æœºåˆ¶ï¼Œé€šè¿‡ç²¾å¿ƒæ„é€ çš„ key æ¥æ§åˆ¶ (å¤ªéš¾)ã€‚
        
        # ç®€å•æ–¹æ¡ˆï¼šç›´æ¥åœ¨è¿™é‡Œæ„é€  EnvStateï¼Œè·³è¿‡ reset çš„éšæœºé€»è¾‘
        # æˆ‘ä»¬éœ€è¦è®¿é—®å…¨å±€çš„ images å’Œ labelsï¼Œè¿™åœ¨ JIT ä¸­æ¯”è¾ƒéº»çƒ¦ã€‚
        # æœ€ä½³æ–¹æ¡ˆï¼šMnistEnv æ”¯æŒç›´æ¥ä¼ å…¥ image_index
        
        # æ—¢ç„¶æˆ‘ä»¬ä¸æƒ³æ”¹ Env å¤ªå¤šï¼Œæˆ‘ä»¬è¿™é‡Œç”¨ä¸€ä¸ªå°æŠ€å·§ï¼š
        # æˆ‘ä»¬æŠŠ images ä¼ ç»™ reset çš„ key? ä¸è¡Œã€‚
        
        # å›é€€ä¸€æ­¥ï¼šæˆ‘ä»¬åœ¨ Python ç«¯å¾ªç¯ï¼Œä¸æŠŠå¾ªç¯ JIT è¿›å»ã€‚
        # è¿™æ ·æˆ‘ä»¬å¯ä»¥æ¯æ¬¡è°ƒç”¨ reset æ—¶ä¼ å…¥ç‰¹å®šçš„æ•°æ®ã€‚
        # ä½†è¿™ä¼šæ…¢ã€‚
        
        # ä¸ºäº†é«˜æ€§èƒ½ï¼Œæˆ‘ä»¬å‡è®¾ MnistEnv.reset(key) ä¸­çš„ key å°±æ˜¯ image_index!
        # æˆ‘ä»¬éœ€è¦ä¿®æ”¹ MnistEnv å—ï¼Ÿæ˜¯çš„ï¼Œå¾®è°ƒä¸€ä¸‹æœ€å¥½ã€‚
        # ä½†ç°åœ¨æˆ‘ä»¬å…ˆå‡è®¾æˆ‘ä»¬å¯ä»¥é€šè¿‡æŸç§æ–¹å¼æ§åˆ¶ã€‚
        
        # [ä¸´æ—¶æ–¹æ¡ˆ] 
        # æˆ‘ä»¬åœ¨ _run_generation å¤–éƒ¨åšå¾ªç¯ï¼
        # è¿™æ · _run_generation åªè·‘ä¸€å¼ å›¾ã€‚
        # ä¸ï¼Œé‚£æ ·æ¢¯åº¦æ›´æ–°å°±å¤ªé¢‘ç¹äº†ã€‚
        
        # [æœ€ç»ˆæ–¹æ¡ˆ]
        # è¿™é‡Œçš„ env_cls æ˜¯ VmapWrapper(MnistEnv)ã€‚
        # æˆ‘ä»¬è°ƒç”¨ conf.env_cls.reset(indices) 
        # æˆ‘ä»¬éœ€è¦ä¿®æ”¹ MnistEnv çš„ reset è®©ä»–æ¥å— indexã€‚
        
        # ä¸ºäº†ä¸å¡åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ç”¨ä¸€ä¸ªå‡è®¾ï¼š
        # eval_batch_indices æ˜¯ (N_imgs, Pop_Size, 2) çš„ Keys
        # æˆ‘ä»¬ç›´æ¥ç”¨è¿™äº› Keys resetã€‚
        # åªè¦å¤–éƒ¨ä¿è¯è¿™äº› Keys å¯¹åº”äº†å¹³è¡¡çš„ 0/1 æ ·æœ¬å³å¯ã€‚
        # ä½† MnistEnv æ˜¯éšæœºå–æ ·...
        
        return carry, None # å ä½

    # --- é‡æ–°è®¾è®¡ ---
    # æˆ‘ä»¬ä¸åœ¨ JIT å†…éƒ¨åšå¤šå›¾å¾ªç¯ï¼Œé‚£å¤ªå¤æ‚äº†ã€‚
    # æˆ‘ä»¬åœ¨ Python ç«¯åšå¾ªç¯ï¼Œç´¯ç§¯æ¢¯åº¦ï¼Œç„¶åæ›´æ–°ä¸€æ¬¡ã€‚
    # è¿™å°±æ˜¯æ ‡å‡†çš„ Batch Gradient Descentã€‚
    
    return runner, None # å ä½ï¼Œè§ä¸‹æ–‡ main å‡½æ•°ä¿®æ”¹

# ==================== [6] æ¢é’ˆ ====================
def probe_network(network, runner, env, key):
    binary_params = jax.tree_util.tree_map(lambda p: p > 0.5, runner.params)
    variables = {'params': binary_params, 'fixed_weights': runner.fixed_weights}
    
    logit_0, logit_1 = None, None
    prob_0, prob_1 = None, None
    
    # æš´åŠ›æœç´¢ 0 å’Œ 1
    rng = key
    for _ in range(50):
        rng, subkey = jax.random.split(rng)
        state = env.reset(subkey) 
        label = int(state.current_label)
        obs = state.obs
        
        carry = network.initial_carry(subkey, 1)
        _, output = network.apply(variables, carry, obs)
        logits = output[0]
        probs = jax.nn.softmax(logits)
        
        if label == 0 and logit_0 is None:
            logit_0, prob_0 = logits, probs
        elif label == 1 and logit_1 is None:
            logit_1, prob_1 = logits, probs
            
        if logit_0 is not None and logit_1 is not None:
            break
            
    return logit_0, prob_0, logit_1, prob_1

# ==================== [7] ä¸»ç¨‹åº ====================
def main():
    print("=== äºŒåˆ†ç±»å¹³è¡¡æ‰¹æ¬¡è®­ç»ƒ (Balanced Batch Training) ===")
    
    # 1. æ•°æ®å‡†å¤‡ (ä¿æŒä¸å˜)
    print(">>> å‡†å¤‡å¹³è¡¡æ•°æ®é›†...")
    images, labels = load_mnist_data('train')
    mask0 = labels == 0
    mask1 = labels == 1
    imgs0, imgs1 = images[mask0], images[mask1]
    
    val_imgs = np.concatenate([imgs0[:16], imgs1[:16]])
    val_labels = np.concatenate([np.zeros(16), np.ones(16)])
    
    # 2. é…ç½®
    K_IN = 100.0
    K_H = 0.5
    K_OUT = 0.1
    
    conf = OmegaConf.create({
        "seed": 42,
        "pop_size": 1024,
        "lr": 0.1,
        "total_generations": 100,
        "batch_size": 16,
        
        # [ä¿®å¤] æ·»åŠ ç¼ºå¤±çš„å‚æ•°
        "eval_size": 128,  # è¯„ä¼°é›†å¤§å°
        "eps": 0.001,      # æ¦‚ç‡æˆªæ–­é˜ˆå€¼
        
        "network_conf": {
            "num_neurons": 509, "excitatory_ratio": 0.76,
            "K_in": K_IN, "K_h": K_H, "K_out": K_OUT, "dt": 0.5
        },
        "use_bio": True, "mix": 0.5
    })
    
    # 3. ç¯å¢ƒä¸ç½‘ç»œ
    # æ³¨æ„ï¼šæˆ‘ä»¬è¿™é‡Œç”¨å…¨é‡æ•°æ®åˆå§‹åŒ– Envï¼Œä½†åœ¨ reset æ—¶æˆ‘ä»¬ä¼š hack å®ƒ
    snn_steps = 200
    # ä»…ä½¿ç”¨ 0/1 æ•°æ®é›†
    all_imgs_2c = np.concatenate([imgs0, imgs1])
    all_lbls_2c = np.concatenate([np.zeros(len(imgs0)), np.ones(len(imgs1))])
    
    base_env = MnistEnv(all_imgs_2c, all_lbls_2c, presentation_steps=snn_steps, dt_ms=0.5)
    base_env.action_size = 2
    env = wrappers.VmapWrapper(base_env)
    
    network_cls = NETWORKS["ConnSNN"]
    tau_vec = None
    prob_mat = None
    if os.path.exists('neuron_physics.npz'):
        phys = np.load('neuron_physics.npz')
        tau_vec = tuple(phys['tau_Vm'].tolist())
        if conf.use_bio and os.path.exists('init_probability.npy'):
            raw = np.load('init_probability.npy')
            prob_mat = conf.mix * raw + (1.0 - conf.mix) * 0.5

    network = network_cls(out_dims=2, tau_Vm_vector=tau_vec, **conf.network_conf)
    
    # 4. ES Setup
    optim = optax.chain(optax.scale_by_adam(), optax.scale(-conf.lr))
    es_conf = ESConfig(
        network_cls=network, optim_cls=optim, env_cls=env,
        pop_size=conf.pop_size, clip_action=False, normalize_obs=False
    )
    
    # åˆå§‹åŒ–
    key_run, key_init = jax.random.split(jax.random.PRNGKey(conf.seed))
    dummy_obs = jnp.zeros((conf.pop_size, 200, 196))
    init_carry = network.initial_carry(key_init, conf.pop_size)
    vars_init = network.init(key_init, init_carry, dummy_obs)
    
    net_params = vars_init['params']
    if prob_mat is not None:
        bio_jnp = jnp.array(prob_mat)
        def _mapper(path, p):
            return bio_jnp if path[-1] == 'kernel_h' else jnp.full_like(p, 0.5)
        net_params = jax.tree_util.tree_map_with_path(_mapper, net_params)
    else:
        net_params = jax.tree_map(lambda x: jnp.full_like(x, 0.5), net_params)
        
    opt_state = optim.init(net_params)
    env_pool = env.reset(jax.random.split(key_init, conf.pop_size)) # Dummy pool
    
    runner = RunnerState(
        key=key_run,
        normalizer_state=running_statistics.init_state(specs.Array((196,), jnp.float32)),
        env_reset_pool=env_pool,
        params=net_params,
        fixed_weights=vars_init['fixed_weights'],
        opt_state=opt_state
    )

    # ================= [å…³é”®] è‡ªå®šä¹‰å•æ­¥è®­ç»ƒå‡½æ•° =================
    # æˆ‘ä»¬å°† _evaluate_step æå–å‡ºæ¥ï¼Œæ”¯æŒæŒ‡å®šè¾“å…¥
    @jax.jit
    def evaluate_batch(params, fixed_weights, batch_obs, batch_labels):
        pop_size = jax.tree_util.tree_leaves(params)[0].shape[0]
        
        # æ‰©å±•è¾“å…¥ä»¥åŒ¹é… PopSize
        # obs: (Pop, Time, Feat)
        obs_broadcast = jnp.repeat(jnp.expand_dims(batch_obs, 0), pop_size, axis=0)
        
        # åˆå§‹åŒ–çŠ¶æ€
        carry = network.initial_carry(jax.random.PRNGKey(0), pop_size)
        
        # [æ ¸å¿ƒä¿®å¤] ä½¿ç”¨ vmap è¿›è¡Œæ‰¹é‡è¯„ä¼°
        # in_axes: 
        #   variables: {'params': 0 (ç§ç¾¤ç»´), 'fixed_weights': None (ä¸åˆ†ç§ç¾¤)}
        #   carry: 0 (æ¯ä¸ªä¸ªä½“æœ‰è‡ªå·±çš„çŠ¶æ€)
        #   x: 0 (æ¯ä¸ªä¸ªä½“æœ‰è‡ªå·±çš„è¾“å…¥)
        vmapped_apply = jax.vmap(
            network.apply, 
            in_axes=({'params': 0, 'fixed_weights': None}, 0, 0)
        )
        
        # è°ƒç”¨ vmap åçš„å‡½æ•°
        _, output = vmapped_apply(
            {'params': params, 'fixed_weights': fixed_weights}, 
            carry, 
            obs_broadcast
        )
        
        # è®¡ç®—å¥–åŠ± (Softmax)
        logits = output - jnp.max(output, axis=-1, keepdims=True)
        probs = jax.nn.softmax(logits)
        
        rewards = probs[:, batch_labels] 
        
        return rewards

    @partial(jax.jit, donate_argnums=(0,))
    def train_step_balanced(runner, batch_imgs, batch_lbls):
        # 1. é‡‡æ ·å‚æ•°
        new_key, run_key = jax.random.split(runner.key)
        runner = runner.replace(key=new_key)
        
        # æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨ es_conf.network_dtype æ˜¯å¯¹çš„
        train_params = _sample_bernoulli_parameter(run_key, runner.params, es_conf.network_dtype, (conf.pop_size - conf.eval_size, ))
        eval_params  = _deterministic_bernoulli_parameter(runner.params, (conf.eval_size, ))
        
        pop_params = jax.tree_util.tree_map(lambda t, e: jnp.concatenate([t, e], axis=0), train_params, eval_params)
        
        # 2. å¾ªç¯è¯„ä¼°
        def _scan_body(cum_fitness, idx):
            img = batch_imgs[idx] 
            lbl = batch_lbls[idx] 
            
            rewards = evaluate_batch(pop_params, runner.fixed_weights, img, lbl)
            return cum_fitness + rewards, None

        total_fitness, _ = jax.lax.scan(_scan_body, jnp.zeros(conf.pop_size), jnp.arange(batch_imgs.shape[0]))
        
        avg_fitness = total_fitness / batch_imgs.shape[0]
        
        # 3. åˆ†å‰² Train/Eval
        fit_train, fit_eval = jnp.split(avg_fitness, [conf.pop_size - conf.eval_size])
        
        # 4. æ¢¯åº¦æ›´æ–°
        weight = _centered_rank_transform(fit_train)
        
        def _nes_grad(p, theta):
            w = weight.reshape((-1,) + (1,) * (theta.ndim - 1)).astype(p.dtype)
            return -jnp.mean(w * (theta.astype(jnp.float32) - p), axis=0)

        grads = jax.tree_util.tree_map(lambda p, theta: _nes_grad(p, theta[:(conf.pop_size - conf.eval_size)]), runner.params, pop_params)
        
        # [ä¿®å¤] ä½¿ç”¨ es_conf.optim_cls è€Œä¸æ˜¯ conf.optim_cls
        updates, new_opt_state = es_conf.optim_cls.update(grads, runner.opt_state, runner.params)
        
        new_params = optax.apply_updates(runner.params, updates)
        new_params = jax.tree_util.tree_map(lambda p: jnp.clip(p, conf.eps, 1 - conf.eps), new_params)
        
        runner = runner.replace(params=new_params, opt_state=new_opt_state)
        
        # è®¡ç®—æ¢¯åº¦æ¨¡é•¿ç”¨äºè¯Šæ–­
        grad_norm = jnp.mean(jnp.abs(grads['kernel_h']))
        
        return runner, jnp.mean(fit_train), jnp.mean(fit_eval), grad_norm
    # --- è®­ç»ƒå¾ªç¯ ---
    print(">>> å¼€å§‹è®­ç»ƒ (Batch Size = 16, Balanced)...")
    pbar = tqdm(range(1, conf.total_generations + 1))
    
    # é¢„å…ˆç”Ÿæˆæ³Šæ¾è„‰å†² (ä¸ºäº†åŠ é€Ÿï¼Œæˆ‘ä»¬ä¸æ¯æ¬¡ resetï¼Œè€Œæ˜¯é‡ç”¨ä¸€ç»„æ•°æ®)
    # æ„é€  16 ä¸ªå›ºå®šçš„æ³Šæ¾åºåˆ—ç”¨äºè®­ç»ƒ (8ä¸ª0, 8ä¸ª1)
    # æ³¨æ„ï¼šåœ¨çœŸå®è®­ç»ƒä¸­åº”è¯¥æ¯ä»£æ¢æ•°æ®ï¼Œä½†ä¸ºäº†è¿‡æ‹Ÿåˆæµ‹è¯•ï¼Œå›ºå®šæ•°æ®æ›´å¥½
    rng_data = jax.random.PRNGKey(999)
    
    def make_poisson_batch(key, imgs, lbls):
        # imgs: (B, 196)
        probs = imgs * (100.0 * 0.5 / 1000.0) # 100Hz
        probs = jnp.expand_dims(probs, 1) # (B, 1, 196)
        probs = jnp.repeat(probs, snn_steps, axis=1) # (B, T, 196)
        spikes = jax.random.bernoulli(key, probs).astype(jnp.float32)
        return spikes, lbls

    train_spikes, train_lbls = make_poisson_batch(rng_data, val_imgs, val_labels) # (16, 200, 196)

    for step in pbar:
        runner, fit, eval_fit, grad = train_step_balanced(runner, train_spikes, train_lbls.astype(jnp.int32))
        
        desc = f"Fit:{fit:.3f} | Eval:{eval_fit:.3f} | Grad:{grad:.5f}"
        
        # æ¢é’ˆ
        if step % 10 == 0:
            l0, p0, l1, p1 = probe_network(network, runner, base_env, jax.random.PRNGKey(step))
            if l0 is not None:
                # è½¬æ¢ä¸º numpy æ–¹ä¾¿æ‰“å°
                l0_np = np.array(l0)
                l1_np = np.array(l1)
                
                diff_0 = l0[0] - l0[1]
                diff_1 = l1[1] - l1[0]
                
                tqdm.write(f"\n[Gen {step} Probe]")
                tqdm.write(f"  Input 0 -> Logits: {l0_np} | Prob(0): {p0[0]:.4f} | Diff: {diff_0:.2f}")
                tqdm.write(f"  Input 1 -> Logits: {l1_np} | Prob(1): {p1[1]:.4f} | Diff: {diff_1:.2f}")
                
                if p0[0] > 0.9 and p1[1] > 0.9:
                    tqdm.write("ğŸš€ 2åˆ†ç±»è®­ç»ƒæˆåŠŸï¼")
                    return

        pbar.set_description(desc)

if __name__ == "__main__":
    main()