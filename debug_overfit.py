import jax
import jax.numpy as jnp
import numpy as np
import optax
from tqdm import tqdm
from flax import linen as nn
from flax.core import freeze

# å¯¼å…¥ç½‘ç»œ
from networks.conn_snn import ConnSNN

# ================= 1. æ¨¡æ‹Ÿç¯å¢ƒ (å›ºå®šæ³Šæ¾è¾“å…¥) =================
class RealisticOverfitEnv:
    def __init__(self, n_features=196, n_steps=200, input_hz=50.0, dt=0.5):
        self.n_steps = n_steps
        # ç”Ÿæˆä¸€ä¸ªå›ºå®šçš„æ³Šæ¾è¾“å…¥åºåˆ—ï¼Œä½œä¸ºâ€œè¿‡æ‹Ÿåˆç›®æ ‡â€
        # å‰åŠéƒ¨åˆ†äº®ï¼ŒååŠéƒ¨åˆ†æš— -> æ ‡ç­¾ 1
        img = np.zeros(n_features)
        img[:n_features//2] = 1.0 
        
        prob = input_hz * (dt/1000.0)
        rng = jax.random.PRNGKey(42)
        
        # é¢„å…ˆç”Ÿæˆè„‰å†²ï¼Œå½¢çŠ¶ (Time, Features)
        probs = jnp.array(img) * prob
        probs = jnp.expand_dims(probs, 0).repeat(n_steps, axis=0)
        self.fixed_obs = jax.random.bernoulli(rng, probs).astype(jnp.float32)
        self.fixed_label = 1

    def get_obs(self):
        return self.fixed_obs, self.fixed_label

# ================= 2. è¾…åŠ©å‡½æ•° =================
def centered_rank(x):
    ranks = jnp.argsort(jnp.argsort(x))
    return (ranks / (len(x) - 1)) - 0.5

# ================= 3. ä¸»ç¨‹åº =================
def main():
    print("=== æ·±åº¦è¯Šæ–­ç‰ˆ Overfit Test (å…¨ç¨‹è¿è¡Œ) ===")
    
    # --- é…ç½® ---
    POP_SIZE = 128
    LR = 0.1
    GENS = 300 # è¿è¡Œå®Œæ•´çš„ 300 ä»£
    
    # å‚æ•°
    K_IN = 15.0
    K_H = 1.0
    K_OUT = 100.0
    
    in_dims = 196
    num_neurons = 509
    exc_ratio = 0.76
    
    print(f"Params: K_in={K_IN}, K_h={K_H}, K_out={K_OUT}")

    # --- æ¨¡å‹ä¸ä¼˜åŒ–å™¨ ---
    tau_vec = tuple([10.0] * num_neurons)
    
    model = ConnSNN(
        out_dims=10,
        num_neurons=num_neurons,
        excitatory_ratio=exc_ratio,
        K_in=K_IN, K_h=K_H, K_out=K_OUT,
        dt=0.5,
        tau_Vm_vector=tau_vec
    )
    
    optimizer = optax.adam(LR)
    
    # åˆå§‹åŒ–
    key = jax.random.PRNGKey(0)
    init_obs = jnp.zeros((200, 196))
    
    # [ä¿®å¤] æ­£ç¡®è°ƒç”¨ initial_carry å¹¶è§£åŒ…
    init_carry_batch = model.initial_carry(key, 1)
    # ä½¿ç”¨ tree_map å»é™¤ batch ç»´åº¦
    init_carry = jax.tree_util.tree_map(lambda x: x[0], init_carry_batch)
    
    variables = model.init(key, init_carry, init_obs)
    
    # æ¦‚ç‡å‚æ•°åˆå§‹åŒ–ä¸º 0.5
    params = jax.tree_map(lambda x: jnp.full_like(x, 0.5), variables['params'])
    fixed_weights = variables['fixed_weights']
    opt_state = optimizer.init(params)
    
    env = RealisticOverfitEnv()
    target_obs, target_label = env.get_obs()
    
    # æ£€æŸ¥è¾“å…¥
    print(f"è¾“å…¥è„‰å†²æ€»æ•°: {jnp.sum(target_obs)}")

    # --- è®­ç»ƒæ­¥ (JIT) ---
    @jax.jit
    def train_step(rng, current_params, opt_state):
        # 1. é‡‡æ ·
        noise_keys = jax.random.split(rng, POP_SIZE)
        
        def sample_mask(p, k):
            return jax.random.uniform(k, p.shape) < p
            
        binary_params = jax.vmap(lambda k: jax.tree_util.tree_map(lambda p: sample_mask(p, k), current_params))(noise_keys)
        
        # 2. è¯„ä¼°
        def evaluate_one(bin_param):
            vars_in = {'params': bin_param, 'fixed_weights': fixed_weights}
            # æ¯æ¬¡è¯„ä¼°é‡æ–°åˆå§‹åŒ–çŠ¶æ€
            carry_batch = model.initial_carry(jax.random.PRNGKey(0), 1)
            carry = jax.tree_util.tree_map(lambda x: x[0], carry_batch)
            
            final_carry, output = model.apply(vars_in, carry, target_obs)
            
            probs = jax.nn.softmax(output)
            acc = (jnp.argmax(output) == target_label).astype(jnp.float32)
            score = probs[target_label]
            
            # ç›‘æ§å‘æ”¾ç‡
            final_rate_mean = jnp.mean(final_carry[2])
            
            return acc, score, output, final_rate_mean

        accs, scores, outputs, rates = jax.vmap(evaluate_one)(binary_params)
        
        # 3. æ¢¯åº¦
        fitness = scores
        fitness_centered = centered_rank(fitness)
        
        def compute_grad(p, theta):
            w_expanded = fitness_centered.reshape((-1,) + (1,) * (p.ndim))
            # [ä¿®å¤] æ˜¾å¼è½¬æ¢ bool -> float32
            return -jnp.mean(w_expanded * (theta.astype(jnp.float32) - p), axis=0)
            
        grads = jax.tree_util.tree_map(lambda p, theta: compute_grad(p, theta), current_params, binary_params)
        
        # 4. æ›´æ–°
        updates, new_opt_state = optimizer.update(grads, opt_state)
        new_params = optax.apply_updates(current_params, updates)
        new_params = jax.tree_util.tree_map(lambda x: jnp.clip(x, 0.001, 0.999), new_params)
        
        grad_norm = jnp.mean(jnp.abs(grads['kernel_h']))
        
        return new_params, new_opt_state, {
            'acc': jnp.mean(accs),
            'score': jnp.mean(scores),
            'rate': jnp.mean(rates),
            'grad_norm': grad_norm,
            'logits_sample': outputs[0]
        }

    # --- å¾ªç¯ ---
    rng = jax.random.PRNGKey(100)
    pbar = tqdm(range(GENS))
    
    solved_once = False
    
    for i in pbar:
        rng, key = jax.random.split(rng)
        params, opt_state, metrics = train_step(key, params, opt_state)
        
        logits = metrics['logits_sample']
        logit_range = jnp.max(logits) - jnp.min(logits)
        
        pbar.set_description(
            f"Acc:{metrics['acc']:.2f} | "
            f"Score:{metrics['score']:.3f} | "
            f"Rate:{metrics['rate']:.4f} | "
            f"GNorm:{metrics['grad_norm']:.5f} | "
            f"LRange:{logit_range:.1f}"
        )
        
        # [ä¿®æ”¹] ä¸é€€å‡ºï¼Œåªæ‰“å°æç¤º
        if metrics['acc'] > 0.98 and not solved_once:
            tqdm.write(f"\nğŸš€ åœ¨ç¬¬ {i} ä»£é¦–æ¬¡æ”¶æ•›ï¼(Accuracy > 98%)")
            solved_once = True

    print("\n=== è®­ç»ƒç»“æŸ ===")
    print(f"Final Accuracy: {metrics['acc']:.4f}")
    print(f"Final Target Score: {metrics['score']:.4f}")
    print(f"Final Logits Sample: {logits}")

if __name__ == "__main__":
    main()