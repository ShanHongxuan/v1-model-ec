# ==================== TF é¢„åˆå§‹åŒ– (ä¸ ec.py ä¿æŒä¸€è‡´) ====================
try:
    from utils.mnist_loader import load_mnist_data
    print(">>> [Init] æ­£åœ¨é¢„åŠ è½½ MNIST æ•°æ®...")
    _ = load_mnist_data()
    print(">>> [Init] MNIST æ•°æ®é¢„åŠ è½½å®Œæˆã€‚")
except (ImportError, ModuleNotFoundError):
    pass
# =======================================================================

import jax
import jax.numpy as jnp
import numpy as np
import optax
from omegaconf import OmegaConf
from tqdm import tqdm
import os

# å¯¼å…¥æ ¸å¿ƒæ¨¡å—
import ec
from networks import NETWORKS
from brax.envs import wrappers 
from envs.mnist_env import MnistEnv
from utils.mnist_loader import load_mnist_data

# ================= è¾…åŠ©å‡½æ•°ï¼šæ¢é’ˆ =================
def probe_logits(network, params, fixed_weights, input_obs):
    """
    ä½¿ç”¨å½“å‰è¿›åŒ–çš„å‚æ•°ï¼ˆå–ç¡®å®šæ€§å‡å€¼ï¼‰è¿è¡Œä¸€æ¬¡ç½‘ç»œï¼Œè·å– Logitsã€‚
    """
    # 1. ç¡®å®šæ€§é‡‡æ ·ï¼šProb > 0.5 è§†ä¸ºè¿æ¥å­˜åœ¨
    # è¿™æ˜¯ ec.py ä¸­ eval_params çš„é€»è¾‘
    binary_params = jax.tree_util.tree_map(lambda p: p > 0.5, params)
    
    # 2. åˆå§‹åŒ–çŠ¶æ€
    key = jax.random.PRNGKey(0)
    # è·å–å•ä¸ª batch çš„ carry (Batch=1)
    init_carry = network.initial_carry(key, 1)
    # å»é™¤ batch ç»´ (vmap å†…éƒ¨é€»è¾‘)
    carry = jax.tree_util.tree_map(lambda x: x[0], init_carry)
    
    # 3. è¿è¡Œå‰å‘ä¼ æ’­
    variables = {'params': binary_params, 'fixed_weights': fixed_weights}
    _, logits = network.apply(variables, carry, input_obs)
    
    return logits

def analyze_logits(logits, target_label):
    """åˆ†æ Logits å¹¶ç»™å‡º K_out å»ºè®®"""
    logits_np = np.array(logits)
    probs = jax.nn.softmax(logits)
    probs_np = np.array(probs)
    
    target_score = logits_np[target_label]
    target_prob = probs_np[target_label]
    
    logit_range = np.max(logits_np) - np.min(logits_np)
    
    print(f"\nğŸ” [Logits æ¢é’ˆ]")
    print(f"    Raw Logits: {np.array2string(logits_np, precision=2, suppress_small=True)}")
    print(f"    Softmax:    {np.array2string(probs_np, precision=4, suppress_small=True)}")
    print(f"    Range: {logit_range:.2f} | Target Prob: {target_prob:.4f}")
    
    # K_out å»ºè®®é€»è¾‘
    if logit_range < 1.0:
        print("    âš ï¸  Logits å·®å¼‚å¤ªå°ï¼Softmax æ— æ³•åŒºåˆ†ã€‚")
        print("    ğŸ‘‰ å»ºè®®: å¢å¤§ K_out (ä¾‹å¦‚ x2 æˆ– x5)")
    elif logit_range > 100.0:
        print("    âš ï¸  Logits å·®å¼‚æå¤§ï¼æ¢¯åº¦å¯èƒ½é¥±å’Œï¼Œä½†é€šå¸¸å¯ä»¥æ¥å—ã€‚")
        print("    ğŸ‘‰ å»ºè®®: ä¿æŒæˆ–ç¨å¾®å‡å° K_out")
    else:
        print("    âœ… Logits èŒƒå›´å¥åº· (ç†æƒ³èŒƒå›´ 2.0 ~ 50.0)ã€‚")

# ================= ä¸»ç¨‹åº =================
def main():
    print("\n=== EC æ¡†æ¶çº§ä¸€è‡´æ€§æµ‹è¯• (å¸¦ Logits æ¢é’ˆ) ===")
    
    # 1. é…ç½® (æ¨¡æ‹Ÿ train_mnist.sh)
    conf = OmegaConf.create({
        "seed": 42,
        "pop_size": 128,  # Debug ç”¨å°ç§ç¾¤
        "eval_size": 32,
        "total_generations": 200,
        
        "lr": 0.1,
        "eps": 0.001,
        "weight_decay": 0.0,
        
        # [è°ƒå‚é‡ç‚¹åŒºåŸŸ]
        "network_conf": {
            "K_in": 0.03,   # è¾“å…¥å¢ç›Š
            "K_h": 0.18,     # é€’å½’å¢ç›Š
            "K_out": 0.01, # è¾“å‡ºå¢ç›Š
            "dt": 0.5,
        },
        
        "episode_conf": {
            "max_episode_length": 1, 
            "action_repeat": 1
        },
        
        "use_bio_probability": True,
        "bio_prob_mix_factor": 0.5,
        "network_type": "ConnSNN",
        
        "es_conf": {},
        "warmup_steps": 0
    })

    # 2. æ•°æ®å‡†å¤‡ (å•å›¾è¿‡æ‹Ÿåˆ)
    all_images, all_labels = load_mnist_data('train')
    target_idx = 7 # é€‰æ‹©æ•°å­— 3 ä½œä¸ºç›®æ ‡
    single_image = all_images[target_idx:target_idx+1]
    single_label = all_labels[target_idx:target_idx+1]
    
    # ç”¨äºæ¢é’ˆçš„å•ä¸ªè§‚æµ‹è¾“å…¥ (å»é™¤ Batch ç»´)
    # MnistEnv ç°åœ¨çš„ reset ç”Ÿæˆ (Batch, Time, Feat)ï¼Œè¿™é‡Œæˆ‘ä»¬æ‰‹åŠ¨æ¨¡æ‹Ÿ
    # æˆ‘ä»¬ç›´æ¥å– imageï¼Œè®©æ¢é’ˆå‡½æ•°é‡Œçš„ network æŠŠå®ƒå½“åšæ¦‚ç‡å›¾å¤„ç† (å¦‚æœæ˜¯ Time-Tensorization æ¨¡å¼)
    # æˆ–è€…æ˜¯ç”Ÿæˆå¥½è„‰å†²ã€‚ä¸ºäº†å‡†ç¡®ï¼Œæˆ‘ä»¬å¤ç”¨ Env çš„é€»è¾‘ç”Ÿæˆä¸€æ¬¡è„‰å†²ã€‚
    dummy_env = MnistEnv(single_image, single_label, presentation_steps=200, dt_ms=0.5)
    probe_state = dummy_env.reset(jax.random.PRNGKey(0))
    probe_obs = probe_state.obs # (Time, 196)

    print(f">>> ç›®æ ‡æ ‡ç­¾: {single_label[0]}")

    # 3. ç¯å¢ƒåˆå§‹åŒ–
    snn_steps = 200
    base_env = MnistEnv(
        images=single_image, 
        labels=single_label, 
        presentation_steps=snn_steps,
        dt_ms=conf.network_conf.dt
    )
    env = wrappers.VmapWrapper(base_env)

    # 4. ç”Ÿç‰©å‚æ•°
    bio_prob_matrix = None
    bio_tau_Vm = None
    if os.path.exists('neuron_physics.npz'):
        physics_data = np.load('neuron_physics.npz')
        conf.network_conf.num_neurons = int(physics_data['num_neurons'])
        conf.network_conf.excitatory_ratio = float(physics_data['excitatory_ratio'])
        bio_tau_Vm = tuple(physics_data['tau_Vm'].tolist())
        
        if conf.use_bio_probability and os.path.exists('init_probability.npy'):
            raw_prob = np.load('init_probability.npy')
            mix = conf.bio_prob_mix_factor
            bio_prob_matrix = mix * raw_prob + (1.0 - mix) * 0.5
            print(f">>> ç”Ÿç‰©æ•°æ®å·²åŠ è½½ (Mix={mix})")

    # 5. ç½‘ç»œåˆå§‹åŒ–
    network_dtype = jnp.float32 
    network_cls = NETWORKS[conf.network_type]
    network_kwargs = {
        "out_dims": env.action_size,
        "neuron_dtype": network_dtype,
        **conf.network_conf
    }
    if bio_tau_Vm:
        network_kwargs["tau_Vm_vector"] = bio_tau_Vm
    
    network = network_cls(**network_kwargs)

    # 6. ESConfig (å…³é”®é…ç½®ä¸€è‡´æ€§)
    p_dtype = jnp.float32
    action_dtype = jnp.float32
    
    optim = optax.chain(
        optax.scale_by_adam(mu_dtype=p_dtype),
        optax.scale(-conf.lr)
    )
    
    es_conf = ec.ESConfig(
        network_cls=network,
        optim_cls=optim,
        env_cls=env,
        pop_size=conf.pop_size,
        lr=conf.lr,
        eps=conf.eps,
        eval_size=conf.eval_size,
        weight_decay=conf.weight_decay,
        warmup_steps=conf.warmup_steps,
        action_dtype=action_dtype,
        p_dtype=p_dtype,
        network_dtype=network_dtype,
        # [å…³é”®] å¿…é¡»æ˜¾å¼å…³é—­ Brax çš„é»˜è®¤å¤„ç†ï¼Œæ‰èƒ½è®© K_out ç”Ÿæ•ˆ
        clip_action=False,
        normalize_obs=False
    )

    # 7. è¿è¡Œå™¨åˆå§‹åŒ–
    key_run, key_network_init = jax.random.split(jax.random.PRNGKey(conf.seed))
    runner = ec._runner_init(key_run, key_network_init, es_conf, init_prob_matrix=bio_prob_matrix)

    # 8. è®­ç»ƒå¾ªç¯
    print(">>> å¼€å§‹è®­ç»ƒ...")
    pbar = tqdm(range(1, conf.total_generations + 1))
    
    for step in pbar:
        runner, metrics = ec._runnner_run(runner, es_conf)
        
        metrics_cpu = jax.device_get(metrics)
        fit = metrics_cpu['fitness']
        eval_fit = metrics_cpu['eval_fitness']
        
        pbar.set_description(f"Eval Acc: {eval_fit:.4f} | Train Fit: {fit:.4f}")
        
        # æ¯ 10 ä»£æˆ–è€…æ˜¯åˆšå¼€å§‹ï¼Œè¿›è¡Œä¸€æ¬¡æ¢é’ˆæ£€æŸ¥
        if step == 1 or step % 20 == 0:
            # è¿™é‡Œçš„ params æ˜¯ runner ä¸­çš„å‡å€¼å‚æ•°
            logits = probe_logits(network, runner.params, runner.fixed_weights, probe_obs)
            analyze_logits(logits, single_label[0])
        
        if eval_fit > 0.99:
            print(f"\nğŸš€ [Success] åœ¨ç¬¬ {step} ä»£è¿‡æ‹ŸåˆæˆåŠŸï¼")
            # æœ€åå†çœ‹ä¸€çœ¼ Logits
            logits = probe_logits(network, runner.params, runner.fixed_weights, probe_obs)
            analyze_logits(logits, single_label[0])
            return

    print("\nâŒ è®­ç»ƒç»“æŸï¼Œæœªèƒ½å®Œå…¨æ”¶æ•›ã€‚")

if __name__ == "__main__":
    main()