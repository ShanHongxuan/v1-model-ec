# ==================== [1] TF é¢„åˆå§‹åŒ– ====================
try:
    from utils.mnist_loader import load_mnist_data
    print(">>> [System] é¢„åŠ è½½ MNIST æ•°æ®...")
    _ = load_mnist_data()
    _TF_PREINIT_SUCCESS = True
except:
    _TF_PREINIT_SUCCESS = False
# =========================================================

import jax
import jax.numpy as jnp
import numpy as np
import pandas as pd # [æ–°å¢] ç”¨äºè¯»å–ç¥ç»å…ƒå…ƒæ•°æ®
import optax
import flax
from omegaconf import OmegaConf
from tqdm import tqdm
from functools import partial
import os
from typing import Any, Tuple, Dict

from networks import NETWORKS
from networks.conn_snn import ConnSNN_Selected
from envs.mnist_env import MnistEnv
from brax.envs import wrappers
from brax.training.acme import running_statistics
from brax.training.acme import specs
from utils.functions import mean_weight_abs

NETWORKS["ConnSNN_Selected"] = ConnSNN_Selected

# ==================== é…ç½®ç±» (ä¿æŒä¸å˜) ====================
@flax.struct.dataclass
class ESConfig:
    network_cls: Any = None
    optim_cls:   Any = None
    env_cls:     Any = None
    pop_size:       int = 2048
    lr:           float = 0.1
    eps:          float = 1e-3
    weight_decay: float = 0.
    warmup_steps:   int = 0
    eval_size:      int = 128
    action_dtype: Any   = jnp.float32
    p_dtype:       Any  = jnp.float32
    network_dtype: Any  = jnp.float32
    clip_action:   bool = False
    normalize_obs: bool = False

@flax.struct.dataclass
class RunnerState:
    key: Any
    normalizer_state: running_statistics.RunningStatisticsState
    env_reset_pool: Any
    params:        Any
    fixed_weights: Any
    opt_state:     Any

@flax.struct.dataclass
class PopulationState:
    network_params: Any
    network_states: Any
    env_states:     Any
    fitness_totrew: jnp.ndarray
    fitness_sum:    jnp.ndarray
    fitness_n:      jnp.ndarray

# ==================== è¾…åŠ©å‡½æ•° (ä¿æŒä¸å˜) ====================
def _centered_rank_transform(x):
    shape = x.shape
    x = x.ravel()
    x = jnp.argsort(jnp.argsort(x))
    x = x / (len(x) - 1) - .5
    return x.reshape(shape)

def _sample_bernoulli_parameter(key, params, dtype, batch_size=()):
    num_vars = len(jax.tree_util.tree_leaves(params))
    treedef = jax.tree_util.tree_structure(params)
    all_keys = jax.random.split(key, num=num_vars)
    return jax.tree_util.tree_map(
        lambda p, k: jax.random.uniform(k, (*batch_size, *p.shape), dtype) < p,
        params, jax.tree_util.tree_unflatten(treedef, all_keys))

def _deterministic_bernoulli_parameter(params, batch_size=()):
    return jax.tree_util.tree_map(lambda p: jnp.broadcast_to(p > 0.5, (*batch_size, *p.shape)), params)

# ==================== è¯„ä¼°æ­¥éª¤ (Evaluate Step) ====================
def _evaluate_step(pop, runner, conf):
    vmapped_apply = jax.vmap(conf.network_cls.apply, ({"params": 0, "fixed_weights": None}, 0, 0))
    new_network_states, act = vmapped_apply(
        {"params": pop.network_params, "fixed_weights": runner.fixed_weights}, 
        pop.network_states, pop.env_states.obs
    )
    new_env_states = conf.env_cls.step(pop.env_states, act)
    new_fitness_totrew = pop.fitness_totrew + new_env_states.reward
    new_fitness_sum    = jnp.where(new_env_states.done, pop.fitness_sum + new_fitness_totrew, pop.fitness_sum)
    new_fitness_n      = jnp.where(new_env_states.done, pop.fitness_n + 1, pop.fitness_n)
    new_fitness_totrew = jnp.where(new_env_states.done, 0, new_fitness_totrew)
    return pop.replace(
        network_states=new_network_states,
        env_states=new_env_states,
        fitness_totrew=new_fitness_totrew,
        fitness_sum=new_fitness_sum,
        fitness_n=new_fitness_n
    )

# ==================== è¯„ä¼°ä¸è®­ç»ƒé€»è¾‘ (ä¿æŒä¸å˜) ====================
@partial(jax.jit, static_argnums=(0,))
def evaluate_batch(network, params, fixed_weights, batch_obs, batch_labels):
    pop_size = jax.tree_util.tree_leaves(params)[0].shape[0]
    obs_broadcast = jnp.repeat(jnp.expand_dims(batch_obs, 0), pop_size, axis=0)
    carry = network.initial_carry(jax.random.PRNGKey(0), pop_size)
    vmapped_apply = jax.vmap(network.apply, in_axes=({'params': 0, 'fixed_weights': None}, 0, 0))
    _, output = vmapped_apply({'params': params, 'fixed_weights': fixed_weights}, carry, obs_broadcast)
    
    logits = output - jnp.max(output, axis=-1, keepdims=True)
    probs = jax.nn.softmax(logits)
    rewards = probs[:, batch_labels] 
    return rewards

@partial(jax.jit, donate_argnums=(0,), static_argnums=(3, 4))
def train_step_balanced(runner, batch_imgs, batch_lbls, es_conf, network):
    conf_pop_size = es_conf.pop_size
    conf_eval_size = es_conf.eval_size
    
    new_key, run_key = jax.random.split(runner.key)
    runner = runner.replace(key=new_key)
    
    # [ä¿®æ­£] ä½¿ç”¨å±€éƒ¨å˜é‡ conf_pop_size å’Œ conf_eval_sizeï¼Œè€Œä¸æ˜¯ conf
    train_params = _sample_bernoulli_parameter(run_key, runner.params, es_conf.network_dtype, (conf_pop_size - conf_eval_size, ))
    eval_params  = _deterministic_bernoulli_parameter(runner.params, (conf_eval_size, )) # è¿™é‡Œä¹‹å‰å†™æˆäº† conf.eval_size
    pop_params = jax.tree_util.tree_map(lambda t, e: jnp.concatenate([t, e], axis=0), train_params, eval_params)
    
    def _scan_body(cum_fitness, idx):
        img = batch_imgs[idx] 
        lbl = batch_lbls[idx] 
        rewards = evaluate_batch(network, pop_params, runner.fixed_weights, img, lbl)
        return cum_fitness + rewards, None

    total_fitness, _ = jax.lax.scan(_scan_body, jnp.zeros(conf_pop_size), jnp.arange(batch_imgs.shape[0]))
    
    avg_fitness = total_fitness / batch_imgs.shape[0]
    
    # [ä¿®æ­£] ä½¿ç”¨å±€éƒ¨å˜é‡ conf_pop_size å’Œ conf_eval_size
    fit_train, fit_eval = jnp.split(avg_fitness, [conf_pop_size - conf_eval_size])
    weight = _centered_rank_transform(fit_train)
    
    def _nes_grad(p, theta):
        w = weight.reshape((-1,) + (1,) * (theta.ndim - 1)).astype(es_conf.p_dtype)
        return -jnp.mean(w * (theta.astype(jnp.float32) - p), axis=0)

    # [ä¿®æ­£] ä½¿ç”¨å±€éƒ¨å˜é‡
    grads = jax.tree_map(lambda p, theta: _nes_grad(p, theta[:(conf_pop_size - conf_eval_size)]), runner.params, pop_params)
    
    updates, new_opt_state = es_conf.optim_cls.update(grads, runner.opt_state, runner.params)
    new_params = optax.apply_updates(runner.params, updates)
    
    # [ä¿®æ­£] ä½¿ç”¨ es_conf.eps è€Œä¸æ˜¯ conf.eps
    new_params = jax.tree_util.tree_map(lambda p: jnp.clip(p, es_conf.eps, 1 - es_conf.eps), new_params)
    
    runner = runner.replace(params=new_params, opt_state=new_opt_state)
    grad_norm = jnp.mean(jnp.abs(grads['kernel_h']))
    
    return runner, jnp.mean(fit_train), jnp.mean(fit_eval), grad_norm

# ==================== [æ¢é’ˆ] ====================
def probe_network(network, runner, env, key):
    binary_params = jax.tree_util.tree_map(lambda p: p > 0.5, runner.params)
    variables = {'params': binary_params, 'fixed_weights': runner.fixed_weights}
    
    # å­˜å‚¨ 0-9 çš„ç»“æœ
    results = {i: {"logit": None, "prob": None, "rate": None} for i in range(10)}
    found_count = 0
    
    rng = key
    # å¢åŠ å°è¯•æ¬¡æ•°ï¼Œç¡®ä¿èƒ½æŠ“åˆ°æ‰€æœ‰ 10 ä¸ªæ•°å­—
    for _ in range(150):
        rng, subkey = jax.random.split(rng)
        state = env.reset(subkey) 
        label = int(state.current_label)
        
        if results[label]["logit"] is None:
            carry = network.initial_carry(subkey, 1)
            # è¿è¡Œç½‘ç»œ
            final_carry, output = network.apply(variables, carry, state.obs)
            
            logits = output[0]
            probs = jax.nn.softmax(logits)
            # è¿™é‡Œçš„ final_carry[2] å¿…é¡»æ˜¯æˆ‘ä»¬åœ¨ networks/conn_snn.py ä¸­ä¿®æ­£è¿‡çš„â€œå…¨æ—¶æ®µå¹³å‡ç‡â€
            avg_rate = jnp.mean(final_carry[2]) 
            
            results[label]["logit"] = logits
            results[label]["prob"] = probs
            results[label]["rate"] = avg_rate
            found_count += 1
            
        if found_count == 10:
            break
            
    return results

# ==================== [è¾…åŠ©] L5 å…´å¥‹æ€§ç¥ç»å…ƒç­›é€‰å™¨ ====================
def get_l5_excitatory_indices(csv_path, total_neurons):
    """
    è¯»å– CSVï¼Œå¤ç°é¢„å¤„ç†æ—¶çš„æ’åºé€»è¾‘ï¼Œå¹¶ç­›é€‰å‡º L5 Excitatory ç¥ç»å…ƒçš„ç´¢å¼•ã€‚
    """
    print(f">>> æ­£åœ¨ç­›é€‰ L5 Excitatory ç¥ç»å…ƒ (From {csv_path})...")
    if not os.path.exists(csv_path):
        print(f"âš ï¸  è­¦å‘Š: æ‰¾ä¸åˆ° {csv_path}ï¼Œå°†å›é€€åˆ°é»˜è®¤çš„å‰ 10 ä¸ªç¥ç»å…ƒã€‚")
        return tuple(range(10))
        
    df = pd.read_csv(csv_path)
    
    # 1. å¤ç° preprocess_data.py çš„æ’åºé€»è¾‘
    # æ˜ å°„ EI ä¸ºæ’åºæƒé‡: E->0, I->1
    # æ³¨æ„ï¼šç¡®ä¿è¿™é‡Œé€»è¾‘ä¸ preprocess_data.py å®Œå…¨ä¸€è‡´
    df['EI_rank'] = df['EI'].map({'E': 0, 'I': 1})
    df_sorted = df.sort_values(['EI_rank', 'simple_id']).reset_index(drop=True)
    
    # éªŒè¯æ•°é‡ä¸€è‡´æ€§
    if len(df_sorted) != total_neurons:
        print(f"âš ï¸  è­¦å‘Š: CSV ç¥ç»å…ƒæ•°é‡ ({len(df_sorted)}) ä¸ ç‰©ç†å‚æ•° ({total_neurons}) ä¸ä¸€è‡´ï¼")
    
    # 2. ç­›é€‰ L5 Excitatory
    # åœ¨ main.py ä¸­ï¼Œl5et å’Œ l5it å±äº 'L5' å±‚ï¼Œtype ä¹Ÿæ˜¯ 'Excitatory' (æˆ–è¢«å½’ç±»ä¸ºE)
    # æˆ‘ä»¬ç›´æ¥ç­›é€‰ layer='L5' ä¸” EI='E'
    # æ³¨æ„ï¼šåˆ—åå¯èƒ½éœ€è¦æ ¹æ®å®é™… csv è°ƒæ•´ï¼Œè¿™é‡Œå‡è®¾æ˜¯ 'layer' å’Œ 'EI'
    l5e_mask = (df_sorted['layer'] == 'L5') & (df_sorted['EI'] == 'E')
    
    l5e_indices = df_sorted[l5e_mask].index.to_numpy()
    
    print(f"    - æ‰¾åˆ° {len(l5e_indices)} ä¸ª L5 Excitatory ç¥ç»å…ƒã€‚")
    print(f"    - ç´¢å¼•èŒƒå›´: {l5e_indices.min()} - {l5e_indices.max()}")
    
    if len(l5e_indices) < 10:
        print("âŒ é”™è¯¯: L5E ç¥ç»å…ƒä¸è¶³ 10 ä¸ªï¼Œæ— æ³•åˆ†é…ç»™ 10 ä¸ªç±»åˆ«ã€‚å›é€€åˆ°é»˜è®¤ã€‚")
        return tuple(range(10))
        
    # 3. é€‰å– 10 ä¸ªä»£è¡¨
    # ç­–ç•¥ï¼šå‡åŒ€é€‰å–ï¼Œä»¥è¦†ç›–ä¸åŒçš„å¾®ç¯è·¯
    selected_indices = np.linspace(0, len(l5e_indices) - 1, 10, dtype=int)
    final_indices = l5e_indices[selected_indices]
    
    print(f"    - é€‰å®š 10 ä¸ªè¯»å‡ºç¥ç»å…ƒ ID: {final_indices}")
    return tuple(final_indices.tolist())

# ==================== [7] ä¸»ç¨‹åº ====================
def main():
    print("=== 10åˆ†ç±» L5Eè¯»å‡º å¹³è¡¡æ‰¹æ¬¡è®­ç»ƒ ===")
    
    # 1. æ•°æ®å‡†å¤‡
    print(">>> å‡†å¤‡ 10 ç±»å¹³è¡¡æ•°æ®é›†...")
    images, labels = load_mnist_data('train')
    class_images = [images[labels == i] for i in range(10)]
    
    imgs_per_class = 8
    val_imgs = np.concatenate([c[:imgs_per_class] for c in class_images])
    val_labels = np.concatenate([np.full(imgs_per_class, i) for i in range(10)])
    
    # 2. åŸºç¡€é…ç½®
    STEPS_PRE = 100; STEPS_STIM = 200; STEPS_RESP = 100
    TOTAL_STEPS = 400
    READOUT_START = 300; READOUT_END = 400
    
    K_IN = 2.0; K_H = 0.05; K_OUT = 20.0
    
    # 3. è·å–ç”Ÿç‰©æ•°æ®ä¸è¯»å‡ºç´¢å¼•
    NEURON_CSV = '../dataset/mice_unnamed/neurons.csv.gz' # å‡è®¾è·¯å¾„
    
    tau_vec = None
    prob_mat = None
    num_neurons_loaded = 509 # é»˜è®¤
    
    if os.path.exists('neuron_physics.npz'):
        phys = np.load('neuron_physics.npz')
        tau_vec = tuple(phys['tau_Vm'].tolist())
        num_neurons_loaded = int(phys['num_neurons'])
        
        # [æ ¸å¿ƒ] è·å– L5E ç´¢å¼•
        l5e_indices = get_l5_excitatory_indices(NEURON_CSV, num_neurons_loaded)
        
        if os.path.exists('init_probability.npy'):
            raw = np.load('init_probability.npy')
            # æš‚æ—¶ä¸åº”ç”¨ mixï¼Œåœ¨ä¸‹é¢åº”ç”¨
    else:
        print("âš ï¸ æœªæ‰¾åˆ° physics æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤ç´¢å¼•ã€‚")
        l5e_indices = tuple(range(10))

    conf = OmegaConf.create({
        "seed": 42,
        "pop_size": 1024,
        "lr": 0.1,
        "total_generations": 500,
        "batch_size": 80,
        "eval_size": 128,
        "eps": 0.001,
        "network_conf": {
            "num_neurons": num_neurons_loaded, 
            "excitatory_ratio": 0.76, # å‡è®¾
            "K_in": K_IN, "K_h": K_H, "K_out": K_OUT, "dt": 0.5,
            
            # [å…³é”®] ä½¿ç”¨ç­›é€‰å‡ºçš„ L5E ç´¢å¼•
            "readout_indices": l5e_indices,
            "readout_start_step": READOUT_START,
            "readout_end_step": READOUT_END
        },
        "use_bio": True, "mix": 0.5
    })
    
    # åŠ è½½æ¦‚ç‡çŸ©é˜µ
    if conf.use_bio and os.path.exists('init_probability.npy'):
        raw = np.load('init_probability.npy')
        prob_mat = conf.mix * raw + (1.0 - conf.mix) * 0.5
        print(f">>> ç”Ÿç‰©æ¦‚ç‡å·²åŠ è½½ (Mix={conf.mix})")

    # 4. ç¯å¢ƒä¸ç½‘ç»œ
    base_env = MnistEnv(
        images, labels, 
        input_hz=200.0, dt_ms=0.5,
        steps_pre_stim=STEPS_PRE, steps_stim=STEPS_STIM, steps_response=STEPS_RESP
    )
    base_env.action_size = 10
    env = wrappers.VmapWrapper(base_env)
    
    network_cls = NETWORKS["ConnSNN_Selected"]
    network = network_cls(out_dims=10, tau_Vm_vector=tau_vec, **conf.network_conf)
    
    # 5. ES Setup
    optim = optax.chain(optax.scale_by_adam(), optax.scale(-conf.lr))
    es_conf = ESConfig(
        network_cls=network, optim_cls=optim, env_cls=env,
        pop_size=conf.pop_size, clip_action=False, normalize_obs=False
    )
    
    # åˆå§‹åŒ–
    key_run, key_init = jax.random.split(jax.random.PRNGKey(conf.seed))
    dummy_obs = jnp.zeros((conf.pop_size, TOTAL_STEPS, 196))
    init_carry = network.initial_carry(key_init, conf.pop_size)
    vars_init = network.init(key_init, init_carry, dummy_obs)
    
    net_params = vars_init['params']
    if prob_mat is not None:
        bio_jnp = jnp.array(prob_mat)
        def _mapper(path, p): return bio_jnp if path[-1] == 'kernel_h' else jnp.full_like(p, 0.5)
        net_params = jax.tree_util.tree_map_with_path(_mapper, net_params)
    else:
        net_params = jax.tree_map(lambda x: jnp.full_like(x, 0.5), net_params)
        
    opt_state = optim.init(net_params)
    env_pool = env.reset(jax.random.split(key_init, conf.pop_size))
    
    runner = RunnerState(
        key=key_run,
        normalizer_state=running_statistics.init_state(specs.Array((196,), jnp.float32)),
        env_reset_pool=env_pool,
        params=net_params,
        fixed_weights=vars_init['fixed_weights'],
        opt_state=opt_state
    )

    # --- è®­ç»ƒå¾ªç¯ ---
    print(">>> å¼€å§‹è®­ç»ƒ...")
    pbar = tqdm(range(1, conf.total_generations + 1))
    
    rng_data = jax.random.PRNGKey(999)
    def make_temporal_batch(key, imgs, lbls):
        B = imgs.shape[0]
        rngs = jax.random.split(key, B)
        def _gen_one(rng, img):
            base = img * (1000.0 * 0.5 / 1000.0)
            silence = jnp.zeros_like(base)
            seq = jnp.concatenate([
                jnp.repeat(jnp.expand_dims(silence, 0), STEPS_PRE, axis=0),
                jnp.repeat(jnp.expand_dims(base, 0), STEPS_STIM, axis=0),
                jnp.repeat(jnp.expand_dims(silence, 0), STEPS_RESP, axis=0)
            ], axis=0)
            return jax.random.bernoulli(rng, seq).astype(jnp.float32)
        spikes = jax.vmap(_gen_one)(rngs, imgs)
        return spikes, lbls

    train_spikes, train_lbls = make_temporal_batch(rng_data, val_imgs, val_labels)

    for step in pbar:
        runner, fit, eval_fit, grad = train_step_balanced(runner, train_spikes, train_lbls.astype(jnp.int32), es_conf, network)
        desc = f"Fit:{fit:.3f} | Eval:{eval_fit:.3f} | Grad:{grad:.5f}"
        
        if step % 20 == 0:
            results = probe_network(network, runner, base_env, jax.random.PRNGKey(step))
            
            # è®¡ç®—æ‘˜è¦æ•°æ®
            correct_count = 0
            total_rate = 0.0
            total_range = 0.0
            
            tqdm.write(f"\n[Gen {step} 10-Class Diagnostic]")
            
            # æ‰“å°ä¸€ä¸ªç®€æ˜“è¡¨æ ¼å¤´
            tqdm.write("Target | Pred | Prob(Self) | Logit(Self) | Rate(Net)")
            tqdm.write("-" * 55)
            
            for i in range(10):
                res = results[i]
                if res["logit"] is not None:
                    l = np.array(res["logit"])
                    p = np.array(res["prob"])
                    r = float(res["rate"])
                    
                    pred = np.argmax(l)
                    if pred == i: correct_count += 1
                    
                    total_rate += r
                    total_range += (np.max(l) - np.min(l))
                    
                    # åªæ‰“å°å‰ 5 ä¸ªæ•°å­—çš„è¯¦ç»†è¡Œï¼Œé¿å…åˆ·å±ï¼Œæœ€åæ±‡æ€»å‡†ç¡®ç‡
                    if i < 10: 
                        tqdm.write(f"  {i}    |  {pred}   |   {p[i]:.4f}   |   {l[i]:.2f}    |  {r:.3f}")
            
            avg_all_rate = total_rate / 10.0
            avg_all_range = total_range / 10.0
            
            tqdm.write("-" * 55)
            tqdm.write(f">>> Summary: Acc:{correct_count/10:.2f} | Avg Range:{avg_all_range:.2f} | Avg Rate:{avg_all_rate:.3f}")
            
            # [å‚æ•°å»ºè®®é€»è¾‘]
            if avg_all_range < 2.0:
                tqdm.write("ğŸ’¡ å»ºè®®: Logit èŒƒå›´å¤ªçª„ -> å¢å¤§ K_out")
            elif avg_all_range > 50.0:
                tqdm.write("ğŸ’¡ æç¤º: Logit èŒƒå›´æå®½ -> å¦‚æœä¸æ”¶æ•›å¯å‡å° K_out")
                
            if avg_all_rate < 0.01:
                tqdm.write("âš ï¸ è­¦å‘Š: å…¨ç½‘é™é»˜ -> å¢å¤§ K_in æˆ– input_hz")
            elif avg_all_rate > 0.4:
                tqdm.write("âš ï¸ è­¦å‘Š: å…¨ç½‘é¥±å’Œ -> å‡å° K_in")

        pbar.set_description(desc)

if __name__ == "__main__":
    main()